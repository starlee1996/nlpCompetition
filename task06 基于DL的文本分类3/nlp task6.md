# Task 6 Transformer & Bert

2群-181-datago-StarLEE

## 1. Transformer原理

Transformer模型的编码部分是一组编码器coder的堆叠，模型的解码部分是由相同数量的解码器decoder的堆叠

编码的部分，编码器结构相同但不共享参数，它们的结构为

Input→Self-Attention→Feed Forward NN

在Self-Attention中使用了多头机制，可以让不同的attention heads关注的部分不同。此外为了保持单词语序，模型还添加了位置编码向量。在Self-Attention和FFNN中都有残差连接以及layer-normalization。

## 2. 基于预训练语言模型的词表示

基于预训练语言模型可以建模上下文信息，从而解决传统静态词向量无法表示一词多义现象的问题。最早建立了两个单向LSTM，将从左到右和从右到左两个方向的隐藏层向量拼接起来学习上下文词嵌入。随后用Transformer代替LSTM，但仅仅使用了单项语言模型，难以建立起上下文信息。最终提出了BERT模型：一个基于Transformer的多层Encoder。

传统方法生成的单词映射表的形式为：先为每一个单词生成一个静态词向量，之后这个单词的表示就被固定住了，不会跟着上下文的变化而做出变化。通过加入语言模型预训练，在下游任务中动态调整Word Embedding，最后输出的词表示能够充分表达单词在特定语境中的语义，进而解决一词多义

GPT是一种生成式预训练模型。GPT开创了NLP界基于预训练-微调的新范式。GPT在第一阶段没有采用两个单向双层LSTM拼接的结构，而是采用基于自回归式的单向语言模型。

BERT也采用了预训练-微调这一两阶段模式，但在模型结构方面，BERT采用了双向语言模型。但在第一阶段的预训练过程中，BERT提出掩码语言模型，通过上下文预测单词本身，这允许模型可以自由编码每个层中来自双向的信息。为了学习词序关系，将Transformer中的三角函数位置表示替换为可学习的参数，其次为了区别单句和双句输入，BERT引入了句子类型表征。为了充分学习句子间的关系，BERT提出了下一个句子预测任务：在训练时，句子对中的第二个句子有50%来自原有的连续句子，其余的50%句子通过在其他句子的随机采样得到。

第二阶段，BERT使用Fine-Tuning模式来微调下游任务。只需要在BERT模型上通过添加Linear分类器即可完成下游任务。

BERT开启了NLP领域“预训练-微调”两阶段的全新范式。第一阶段，在海量无标注文本上预训练一个双向语言模型，将Transformer作为特征提取器在解决并行性和长距离依赖问题上都要领先于传统的RNN或CNN，通过预训练，可以将训练数据中的词法，句法，语法知识以网络参数的形式提炼到模型当中，在第二阶段使用下游任务的数据Fine-tuning不同层数的BERT模型参数，或把BERT当作特征提取器生成BERT Embedding，作为新特征引入下游任务。

## 3. 基于BERT的文本分类

### 3.1 Bert Pretrain



