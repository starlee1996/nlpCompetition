# Task 5 Word2Vec

2群-181-datago-StarLEE

## 1. Word2Vec简介

Word2Vec模型基本思想是对出现在上下文环境里的词进行预测。对于每条输入文本，选取一个上下文窗口和一个中心词，并基于中心词去预测窗口里其他词出现的概率。Word2Vec模型可以方便地从新增语料中学习到新增词的向量表达，是一种高效的在线学习算法。

Word2Vec分为两个部分：建立模型与通过模型获取嵌入词向量。Word2Vec与auto encoder的思想很相似，先基于训练数据构建一个神经网络，当这个模型训练好以后，并不用训练好的模型处理新任务，而是获取该模型训练所得的参数，例如隐层的权重矩阵。这些权重在Word2Vec中实际上就是试图去学习的“word vectors”。

## 2. skip-grams

skip-grams为输入一个词，借助NN输出上下文内容

+ input word：输入的中心词
+ skip_window：中心词左右选取词（窗口）容量
+ num_skips：窗口中选取不同词作为output的数量

### 2.1 skip-grams训练

Word2Vec是一个规模庞大的NN，为了降低计算量，有以下解决办法：

1.常见单词组合视为一个单词

2.对高频单词进行抽样以减少训练样本个数：

高频单词存在问题：

+ 得到关于高频单词的成对训练样本时，该类样本基本不会提供其中低频单词的语义信息
+ 高频单词会产生大量包含它的样本，而样本数量远远超过学习高频单词词向量所需的样本数

通过抽样来解决：原始文本中的每个词都有一定概率从文本中删去，这个概率与词频有关：

![](https://camo.githubusercontent.com/eefc50490ed095d0f01b6e94f723c3365ce3f00b/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f32303230303731343230353435363839382e706e67)

3.对优化目标采用“negative sampling”方法，使得每个训练样本只会更新一小部分的模型权重，以降低计算量







## 3. Hierarchical Softmax

### 3.1 霍夫曼树

